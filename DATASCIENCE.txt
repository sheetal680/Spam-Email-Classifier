import pandas as pd
import numpy as np
import re
import string
import pickle
import matplotlib.pyplot as plt
import seaborn as sns

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download('stopwords')

from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# ========================
# 1. Data Loading
# ========================
# Update the path to your SMSSpamCollection file
df = pd.read_csv(r"C:\Users\mohan\OneDrive\Desktop\ds project\SMSSpamCollection.csv", 
                 sep='\t', header=None, names=['label', 'message'])
print("Dataset shape:", df.shape)
print("Label distribution:\n", df['label'].value_counts())
print(df.head())

# ========================
# 2. Text Preprocessing Function
# ========================
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess_text(text):
    # Lowercase
    text = text.lower()
    # Remove punctuation
    text = text.translate(str.maketrans('', '', string.punctuation))
    # Tokenize by splitting on whitespace
    tokens = text.split()
    # Remove stopwords and perform stemming
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    # Return tokens joined back to a string (needed by TfidfVectorizer)
    return " ".join(tokens)

# Apply preprocessing
df['clean_message'] = df['message'].apply(preprocess_text)

# ========================
# 3. Splitting the Data
# ========================
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_message'], df['label'], test_size=0.2, random_state=42, stratify=df['label'])

# ========================
# 4. Building & Tuning the Pipeline Model with Logistic Regression
# ========================
# Pipeline: TF-IDF vectorization and Logistic Regression classifier.
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer()),
    ('clf', LogisticRegression(max_iter=1000, solver='liblinear'))
])

# Define hyperparameter grid for grid search
param_grid = {
    'tfidf__ngram_range': [(1,1), (1,2)],   # Unigrams vs. unigrams + bigrams
    'tfidf__min_df': [1, 2],
    'clf__C': [0.1, 1, 10],                  # Inverse regularization strength
    'clf__penalty': ['l1', 'l2']             # L1 can induce sparsity; L2 is standard
}

# Set up GridSearchCV to optimize parameters based on accuracy
grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, verbose=1)
grid_search.fit(X_train, y_train)

print("Best Parameters from GridSearchCV:")
print(grid_search.best_params_)

# ========================
# 5. Evaluation on Test Set
# ========================
best_model = grid_search.best_estimator_
predictions = best_model.predict(X_test)

accuracy = accuracy_score(y_test, predictions)
conf_matrix = confusion_matrix(y_test, predictions)
class_report = classification_report(y_test, predictions)

print("Test Accuracy:", accuracy)
print("Confusion Matrix:\n", conf_matrix)
print("Classification Report:\n", class_report)

# Visualization of confusion matrix
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, fmt="d", cmap="Blues",
            xticklabels=['ham', 'spam'], yticklabels=['ham', 'spam'])
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix - Logistic Regression Spam Filter')
plt.show()

# ========================
# 6. Cross-Validation on Entire Dataset
# ========================
cv_scores = cross_val_score(best_model, df['clean_message'], df['label'], cv=5)
print("Cross-Validation Accuracy Scores:", cv_scores)
print("Mean CV Accuracy:", cv_scores.mean())

# ========================
# 7. Saving the Optimized Model
# ========================
with open('optimized_logreg_spam_filter.pkl', 'wb') as f:
    pickle.dump(best_model, f)
print("Optimized model saved as optimized_logreg_spam_filter.pkl")

# ========================
# 8. Flask API Deployment Outline
# ========================
# Save the following code into a separate file (e.g., app.py) to deploy via Flask.
"""
from flask import Flask, request, jsonify
import pickle
import nltk, string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
nltk.download('stopwords')

# Preprocessing function as defined earlier
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()
def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = text.split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

# Load the optimized model
with open('optimized_logreg_spam_filter.pkl', 'rb') as f:
    model = pickle.load(f)

app = Flask(_name_)

@app.route('/predict', methods=['POST'])
def predict():
    # Expecting JSON payload with a 'message' key
    data = request.get_json(force=True)
    message = data.get('message', '')
    processed_message = preprocess_text(message)
    prediction = model.predict([processed_message])[0]
    return jsonify({'prediction': prediction})

if _name_ == '_main_':
    app.run(debug=True)
"""
%%writefile app.py
import streamlit as st
import pickle
import nltk
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer

# Download NLTK stopwords if needed (silently)
nltk.download('stopwords', quiet=True)

# Preprocessing function (must match the one used when training your model)
stop_words = set(stopwords.words('english'))
stemmer = PorterStemmer()

def preprocess_text(text):
    text = text.lower()
    text = text.translate(str.maketrans('', '', string.punctuation))
    tokens = text.split()
    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]
    return " ".join(tokens)

# Load your saved model once at app startup
# (Make sure 'optimized_logreg_spam_filter.pkl' is in the same directory)
@st.cache_resource
def load_model():
    with open('optimized_logreg_spam_filter.pkl', 'rb') as f:
        return pickle.load(f)

model = load_model()

# Streamlit UI
st.title("SMS Spam Filter")
st.write("Enter an SMS message below to predict if it's spam or ham:")

user_input = st.text_area("SMS Message", value="Type your message here...")

if st.button("Predict"):
    processed_input = preprocess_text(user_input)
    prediction = model.predict([processed_input])[0]
    st.write(f"*Prediction:* {prediction}")